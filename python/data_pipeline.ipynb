{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac8eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to OSHE_Analytics successful.\n",
      "Target Staging: staging.accidents_raw\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, zipfile, io, os, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. DATABASE CONFIGURATION ---\n",
    "DB_CONFIG = {\n",
    "    'server': 'DESKTOP-PG3CATF',\n",
    "    'database': 'OSHE_Analytics',\n",
    "    'driver': 'ODBC Driver 18 for SQL Server'\n",
    "}\n",
    "\n",
    "# Tambahkan definisi tabel di sini agar mudah diatur (Centralized Mapping)\n",
    "STAGING_TABLE = 'staging.accidents_raw'\n",
    "PRODUCTION_TABLE = 'dbo.Accidents_Clustered_Final'\n",
    "FINAL_TABLE = 'dbo.Accidents_Clustered_Final'\n",
    "\n",
    "# --- 2. DIRECTORY STRUCTURE ---\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'python' else Path.cwd()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "STAGING_DIR = BASE_DIR / 'data' / 'staging'\n",
    "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
    "\n",
    "# Pastikan folder fisik tersedia di Windows Explorer\n",
    "for folder in [RAW_DIR, STAGING_DIR, PROCESSED_DIR]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 3. CONNECTION HELPER ---\n",
    "def get_conn_str(db='master'):\n",
    "    \"\"\"Fungsi untuk membuat koneksi string secara dinamis.\"\"\"\n",
    "    target_db = db if db == 'master' else DB_CONFIG['database']\n",
    "    return (f\"DRIVER={{{DB_CONFIG['driver']}}};\"\n",
    "            f\"SERVER={DB_CONFIG['server']};\"\n",
    "            f\"DATABASE={target_db};\"\n",
    "            f\"Trusted_Connection=yes;TrustServerCertificate=yes;\")\n",
    "\n",
    "# --- 4. HEALTH CHECK ---\n",
    "try:\n",
    "    # Menggunakan 'db' untuk memanggil database utama di DB_CONFIG\n",
    "    with pyodbc.connect(get_conn_str(db='db'), timeout=5) as conn:\n",
    "        print(f\"Connection to {DB_CONFIG['database']} successful.\")\n",
    "        print(f\"Target Staging: {STAGING_TABLE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 2: Syncing Data from MSHA Official Source\")\n",
    "\n",
    "MSHA_URL = \"https://arlweb.msha.gov/OpenGovernmentData/DataSets/Accidents.zip\"\n",
    "ZIP_PATH = RAW_DIR / 'accidents_raw.zip'\n",
    "\n",
    "try:\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(MSHA_URL, headers=headers, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Save ZIP binary\n",
    "        with open(ZIP_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Extract content\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(RAW_DIR)\n",
    "            extracted_files = zip_ref.namelist()\n",
    "        \n",
    "        print(f\"Extraction Complete. Files: {', '.join(extracted_files)}\")\n",
    "        print(f\"Payload Size: {len(response.content) / (1024**2):.2f} MB\")\n",
    "    else:\n",
    "        print(f\"Source unavailable. HTTP Status: {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Critical error during acquisition: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea2d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Data Ingestion & Schema Validation\n",
      "Processing source: Accidents.txt (214.66 MB)\n",
      "Ingestion Success.\n",
      "Dimensions: 270,921 rows | 57 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Data Ingestion & Schema Validation\")\n",
    "\n",
    "# Locate the extracted TXT file\n",
    "target_files = list(RAW_DIR.glob('Accidents.[tT][xX][tT]'))\n",
    "\n",
    "if not target_files:\n",
    "    raise FileNotFoundError(\"Target file 'Accidents.txt' not found in raw directory.\")\n",
    "\n",
    "data_source = target_files[0]\n",
    "print(f\"Processing source: {data_source.name} ({data_source.stat().st_size / (1024**2):.2f} MB)\")\n",
    "\n",
    "try:\n",
    "    # MSHA standard uses pipe delimiter\n",
    "    df = pd.read_csv(data_source, sep='|', encoding='latin-1', low_memory=False, on_bad_lines='skip')\n",
    "    \n",
    "    print(\"Ingestion Success.\")\n",
    "    print(f\"Dimensions: {df.shape[0]:,} rows | {df.shape[1]} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ingestion failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a7c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 : Data Profiling and Staging Process\n",
      "------------------------------\n",
      "Dataset Profile for: Accidents.txt\n",
      "Total Records  : 270,921\n",
      "Total Features : 57\n",
      "Memory Usage   : 679.13 MB\n",
      "------------------------------\n",
      "\n",
      "Top Missing Values:\n",
      " - CONTRACTOR_ID       : 242,826 (89.6%)\n",
      " - INVEST_BEGIN_DT     : 216,502 (79.9%)\n",
      " - EQUIP_MODEL_NO      : 163,123 (60.2%)\n",
      " - CLOSED_DOC_NO       : 150,913 (55.7%)\n",
      " - SCHEDULE_CHARGE     : 65,021 (24.0%)\n",
      " - DAYS_RESTRICT       : 61,957 (22.9%)\n",
      " - DAYS_LOST           : 46,418 (17.1%)\n",
      " - TOT_EXPER           : 44,004 (16.2%)\n",
      " - RETURN_TO_WORK_DT   : 42,791 (15.8%)\n",
      " - MINE_EXPER          : 41,117 (15.2%)\n",
      "Process Completed. Staging saved to: e:\\MyProject-GitHub\\OSHE-Interview-Project\\data\\staging\\accidents_staging.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4 : Data Profiling and Staging Process\")\n",
    "\n",
    "# 1. Pipeline pembacaan data (Re-using logic from Step 3)\n",
    "try:\n",
    "    # Mengambil file TXT pertama yang ditemukan\n",
    "    target_path = next(RAW_DIR.glob('Accidents.[tT][xX][tT]'))\n",
    "    \n",
    "    df = pd.read_csv(target_path, sep='|', encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    # 2. Dataset Profiling\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Dataset Profile for: {target_path.name}\")\n",
    "    print(f\"Total Records  : {len(df):,}\")\n",
    "    print(f\"Total Features : {len(df.columns)}\")\n",
    "    print(f\"Memory Usage   : {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Analisis Kualitas Data (Missing Values)\n",
    "    null_summary = df.isnull().sum()\n",
    "    top_missing = null_summary[null_summary > 0].sort_values(ascending=False).head(10)\n",
    "    \n",
    "    if not top_missing.empty:\n",
    "        print(\"\\nTop Missing Values:\")\n",
    "        for col, count in top_missing.items():\n",
    "            print(f\" - {col:<20}: {count:,} ({ (count/len(df))*100:.1f}%)\")\n",
    "\n",
    "    # 4. Save to Staging Area\n",
    "    STAGING_FILE = STAGING_DIR / 'accidents_staging.csv'\n",
    "    df.to_csv(STAGING_FILE, index=False)\n",
    "    \n",
    "    # 5. Export Data Dictionary Singkat\n",
    "    DICT_FILE = STAGING_DIR / 'data_info.txt'\n",
    "    with open(DICT_FILE, 'w') as f:\n",
    "        f.write(f\"Extraction Timestamp: {datetime.now()}\\n\")\n",
    "        f.write(f\"Total Rows: {len(df)}\\n\\n\")\n",
    "        f.write(\"Columns List:\\n\" + \"\\n\".join([f\"- {c}\" for c in df.columns]))\n",
    "\n",
    "    print(f\"Process Completed. Staging saved to: {STAGING_FILE}\")\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"Error: Accidents.txt tidak ditemukan di folder raw.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dec4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Database Profiling & Integrity Audit\n",
      "Data Ingested: 270,921 records from staging.accidents_raw\n",
      "\n",
      "[Schema Audit]\n",
      "object            57\n",
      "int64              1\n",
      "datetime64[ns]     1\n",
      "\n",
      "[Integrity Check]\n",
      "  - Duplicate Records: 0\n",
      "  - Clean Records: 270,921\n",
      "\n",
      "[Metrics Summary - Top 10 Features]\n",
      "            mean  min       max\n",
      "STG_ID  135461.0  1.0  270921.0\n",
      "Validation snapshot saved to: e:\\MyProject-GitHub\\OSHE-Interview-Project\\data\\staging\\data_sample.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Database Profiling & Integrity Audit\")\n",
    "\n",
    "try:\n",
    "    # 1. Data Retrieval - Menggunakan 'with' agar koneksi otomatis tertutup\n",
    "    conn_str = get_conn_str(db='OSHE_Analytics') \n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        query = f\"SELECT * FROM {STAGING_TABLE}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "\n",
    "    print(f\"Data Ingested: {len(df):,} records from {STAGING_TABLE}\")\n",
    "\n",
    "    # 2. Schema Audit\n",
    "    # Menampilkan ringkasan tipe data yang masuk ke DataFrame\n",
    "    print(\"\\n[Schema Audit]\")\n",
    "    print(df.dtypes.value_counts().to_string())\n",
    "\n",
    "    # 3. Data Integrity Check\n",
    "    # Mengecek apakah ada baris yang sama persis (duplikat)\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n[Integrity Check]\")\n",
    "    print(f\"  - Duplicate Records: {duplicates:,}\")\n",
    "    print(f\"  - Clean Records: {len(df) - duplicates:,}\")\n",
    "\n",
    "    # 4. Statistical Summary (Numeric Only)\n",
    "    # Mengambil kolom angka dan menampilkan statistik deskriptif dasar\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    if not numeric_cols.empty:\n",
    "        print(\"\\n[Metrics Summary - Top 10 Features]\")\n",
    "        print(numeric_cols.describe().T[['mean', 'min', 'max']].head(10))\n",
    "\n",
    "    # 5. Export Snapshot for Validation\n",
    "    # Menyimpan 100 baris pertama untuk keperluan inspeksi manual/Power BI preview\n",
    "    SAMPLE_PATH = STAGING_DIR / 'data_sample.csv'\n",
    "    df.head(100).to_csv(SAMPLE_PATH, index=False)\n",
    "    print(f\"Validation snapshot saved to: {SAMPLE_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Profiling Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81676048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 6: Defining OSHE Analytical Scope & Cleaning Strategy\n",
      "Strategy Initialized. Monitoring 17 critical features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cell 6: Defining OSHE Analytical Scope & Cleaning Strategy\")\n",
    "\n",
    "# Fokus pada fitur yang memiliki dampak langsung pada keselamatan (Safety Impact)\n",
    "FEATURE_GROUPS = {\n",
    "    'Core': ['DOCUMENT_NO', 'MINE_ID', 'OPERATOR_ID'],\n",
    "    'Risk_Context': ['ACCIDENT_DT', 'CAL_YR', 'SUBUNIT', 'CLASSIFICATION', 'ACCIDENT_TYPE'],\n",
    "    'Severity_Metrics': ['DEGREE_INJURY_CD', 'DAYS_LOST', 'SCHEDULE_CHARGE', 'NO_INJURIES'],\n",
    "    'Human_Factors': ['OCCUPATION', 'ACTIVITY', 'TOT_EXPER', 'MINE_EXPER', 'JOB_EXPER']\n",
    "}\n",
    "\n",
    "# Mapping kategori untuk konversi tipe data otomatis\n",
    "CLEANING_CONFIG = {\n",
    "    'to_numeric': ['CAL_YR', 'CAL_QTR', 'NO_INJURIES', 'TOT_EXPER', 'DAYS_LOST', 'SCHEDULE_CHARGE'],\n",
    "    'to_string': ['DEGREE_INJURY_CD', 'CLASSIFICATION_CD', 'ACCIDENT_TYPE_CD', 'MINE_ID'],\n",
    "    'missing_threshold': 0.70  # Buang kolom jika data kosong > 70%\n",
    "}\n",
    "\n",
    "all_important_cols = [col for sublist in FEATURE_GROUPS.values() for col in sublist]\n",
    "print(f\"Strategy Initialized. Monitoring {len(all_important_cols)} critical features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f632195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 7: Executing Data Cleaning Pipeline\n",
      "Cleaning Complete. Remaining Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Cell 7: Executing Data Cleaning Pipeline\")\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Dimensionality Reduction (Drop High Missing)\n",
    "null_pct = df_clean.isnull().mean()\n",
    "cols_to_drop = [c for c in df_clean.columns if null_pct[c] > CLEANING_CONFIG['missing_threshold'] \n",
    "                and c not in all_important_cols]\n",
    "df_clean.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# 2. Type Casting\n",
    "for col in CLEANING_CONFIG['to_numeric']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').fillna(0)\n",
    "\n",
    "# 3. Text Standardization\n",
    "# Membersihkan spasi dan mengubah 'nan' atau '?' menjadi 'Unknown' secara massal\n",
    "obj_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "df_clean[obj_cols] = df_clean[obj_cols].astype(str).apply(lambda x: x.str.strip())\n",
    "df_clean.replace(['nan', '?', 'None', 'No Value Found'], 'Unknown', inplace=True)\n",
    "\n",
    "# 4. Feature Engineering (New Insights)\n",
    "if 'ACCIDENT_DT' in df_clean.columns:\n",
    "    dt_series = pd.to_datetime(df_clean['ACCIDENT_DT'], errors='coerce')\n",
    "    df_clean['MONTH'] = dt_series.dt.month\n",
    "    df_clean['DAY_OF_WEEK'] = dt_series.dt.day_name()\n",
    "\n",
    "# Menghitung SEVERITY_SCORE sebagai kombinasi hari hilang dan denda jadwal\n",
    "df_clean['SEVERITY_SCORE'] = df_clean['DAYS_LOST'] + df_clean['SCHEDULE_CHARGE']\n",
    "df_clean['IS_FATAL'] = (df_clean['DEGREE_INJURY_CD'] == '01').astype(int)\n",
    "\n",
    "print(f\"Cleaning Complete. Remaining Nulls: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "988badad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 8: Preparing Features for Machine Learning (Clustering)\n",
      "Features Ready: 8 variables scaled.\n",
      "     CAL_YR     MONTH  SEVERITY_SCORE  IS_FATAL  NO_INJURIES  TOT_EXPER  \\\n",
      "0  0.467442  0.481290       -0.139006 -0.066533     0.154529  -0.760775   \n",
      "1  2.002024 -0.714448       -0.139006 -0.066533     0.154529   1.417391   \n",
      "2 -0.648618  1.378094       -0.139006 -0.066533     0.154529   2.395987   \n",
      "\n",
      "   MINE_EXPER  JOB_EXPER  \n",
      "0   -0.990713  -1.039982  \n",
      "1    0.906909   1.173644  \n",
      "2    0.645008   0.648729  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "print(\"Cell 8: Preparing Features for Machine Learning (Clustering)\")\n",
    "\n",
    "# 1. Feature Selection\n",
    "# Pilih fitur numerik dan kategorik yang paling berpengaruh pada pola kecelakaan\n",
    "CLUSTER_FEATURES = [\n",
    "    'CAL_YR', 'MONTH', 'SEVERITY_SCORE', 'IS_FATAL', \n",
    "    'NO_INJURIES', 'TOT_EXPER', 'MINE_EXPER', 'JOB_EXPER'\n",
    "]\n",
    "\n",
    "df_ml = df_clean[CLUSTER_FEATURES].copy()\n",
    "\n",
    "# 2. Categorical Encoding\n",
    "# Mengubah teks menjadi angka karena algoritma clustering hanya menerima angka\n",
    "le = LabelEncoder()\n",
    "for col in df_ml.select_dtypes(include=['object']).columns:\n",
    "    df_ml[col] = le.fit_transform(df_ml[col])\n",
    "\n",
    "# 3. Feature Scaling (Penting untuk K-Means)\n",
    "# Menyamakan skala data (misal: Tahun 2024 vs Jumlah Cedera 1) agar tidak jomplang\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_ml),\n",
    "    columns=df_ml.columns\n",
    ")\n",
    "\n",
    "print(f\"Features Ready: {df_scaled.shape[1]} variables scaled.\")\n",
    "print(df_scaled.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a10de930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 9: Execute Optimized Clustering Pipeline\n",
      " - Testing k=3: Silhouette Score = 0.258\n",
      " - Testing k=4: Silhouette Score = 0.266\n",
      " - Testing k=5: Silhouette Score = 0.267\n",
      " - Testing k=6: Silhouette Score = 0.239\n",
      "\n",
      "✓ Optimal Cluster Selected: 5\n",
      "\n",
      "[Cluster Distribution]\n",
      "CLUSTER\n",
      "3    47.09%\n",
      "0    28.31%\n",
      "1    23.74%\n",
      "4     0.44%\n",
      "2     0.41%\n",
      "Name: proportion, dtype: object\n",
      "✅ Blank Labels Fixed. Unique Labels: ['Low Risk - New Workers' 'Low Risk - Experienced' 'Moderate Severity'\n",
      " 'High Risk - Severe Injuries' 'CRITICAL - Fatal Risk']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"Cell 9: Execute Optimized Clustering Pipeline\")\n",
    "\n",
    "# 1. Hyperparameter Tuning (Elbow & Silhouette Method) via Sampling\n",
    "# Menggunakan 20k sample agar proses pencarian 'k' cepat namun tetap representatif\n",
    "df_sample = df_scaled.sample(n=min(20000, len(df_scaled)), random_state=42)\n",
    "k_results = []\n",
    "\n",
    "for k in range(3, 7):\n",
    "    model = KMeans(n_clusters=k, n_init=5, max_iter=100, random_state=42)\n",
    "    labels = model.fit_predict(df_sample)\n",
    "    sil = silhouette_score(df_sample, labels, sample_size=5000)\n",
    "    k_results.append((k, model.inertia_, sil))\n",
    "    print(f\" - Testing k={k}: Silhouette Score = {sil:.3f}\")\n",
    "\n",
    "# 2. Select Best K and Fit to Full Dataset\n",
    "best_k = max(k_results, key=lambda x: x[2])[0]\n",
    "print(f\"\\n✓ Optimal Cluster Selected: {best_k}\")\n",
    "\n",
    "# Fitting model final ke seluruh data (270k+ baris)\n",
    "final_kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "df_clean['CLUSTER'] = final_kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# 3. Quick Distribution Check\n",
    "print(\"\\n[Cluster Distribution]\")\n",
    "print(df_clean['CLUSTER'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "\n",
    "# 4. Mapping Cluster ke Label Bisnis yang Informatif\n",
    "cluster_map = {\n",
    "    0: 'Moderate Severity',\n",
    "    1: 'Low Risk - Experienced',\n",
    "    2: 'High Risk - Severe Injuries',\n",
    "    3: 'Low Risk - New Workers',\n",
    "    4: 'CRITICAL - Fatal Risk'\n",
    "}\n",
    "\n",
    "# Membuat kolom label dan mengisi nilai yang kosong (jika ada) dengan 'Unclassified'\n",
    "df_clean['CLUSTER_LABEL'] = df_clean['CLUSTER'].map(cluster_map).fillna('Unclassified')\n",
    "\n",
    "# Menentukan RISK_LEVEL secara otomatis\n",
    "def assign_risk(cluster):\n",
    "    if cluster == 4: return 'Critical'\n",
    "    if cluster in [0, 2]: return 'High'\n",
    "    return 'Low'\n",
    "\n",
    "df_clean['RISK_LEVEL'] = df_clean['CLUSTER'].apply(assign_risk)\n",
    "\n",
    "# FINAL CHECK: Pastikan tidak ada kolom penting yang masih Null\n",
    "df_final = df_clean.fillna('Unknown')\n",
    "\n",
    "print(f\"✅ Blank Labels Fixed. Unique Labels: {df_final['CLUSTER_LABEL'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7290ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 10: Loading Clustered Insights to Production Table\n",
      "Inserting 270,921 rows...\n",
      "Production Table Created: dbo.Accidents_Clustered_Final\n"
     ]
    }
   ],
   "source": [
    "print(\"Cell 10: Loading Clustered Insights to Production Table\")\n",
    "\n",
    "# 1. Feature Selection for Production\n",
    "# Memilih kolom asli ditambah label cluster untuk kebutuhan Dashboard Power BI\n",
    "export_cols = all_important_cols + ['YEAR', 'MONTH', 'DAY_OF_WEEK', 'SEVERITY_SCORE', 'IS_FATAL', 'CLUSTER']\n",
    "df_final = df_clean[[c for c in export_cols if c in df_clean.columns]].copy()\n",
    "\n",
    "# 2. SQL Production Schema Handling\n",
    "try:\n",
    "    with pyodbc.connect(get_conn_str()) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Reset table jika sudah ada (Full Refresh)\n",
    "        cursor.execute(f\"IF OBJECT_ID('{PRODUCTION_TABLE}') IS NOT NULL DROP TABLE {PRODUCTION_TABLE}\")\n",
    "        \n",
    "        # Logic sederhana pembuatan tabel: Semua string jadi NVARCHAR, angka jadi FLOAT/INT\n",
    "        cols_sql = []\n",
    "        for col, dtype in df_final.dtypes.items():\n",
    "            sql_type = \"INT\" if \"int\" in str(dtype) else \"FLOAT\" if \"float\" in str(dtype) else \"NVARCHAR(MAX)\"\n",
    "            cols_sql.append(f\"[{col}] {sql_type}\")\n",
    "            \n",
    "        cursor.execute(f\"CREATE TABLE {PRODUCTION_TABLE} ({', '.join(cols_sql)})\")\n",
    "        conn.commit()\n",
    "        \n",
    "        # 3. Batch Loading (Optimized for 270k records)\n",
    "        print(f\"Inserting {len(df_final):,} rows...\")\n",
    "        placeholders = \", \".join([\"?\"] * len(df_final.columns))\n",
    "        sql_insert = f\"INSERT INTO {PRODUCTION_TABLE} VALUES ({placeholders})\"\n",
    "        \n",
    "        # Menggunakan list of tuples untuk kecepatan insert (executemany)\n",
    "        data_tuples = [tuple(x) for x in df_final.values]\n",
    "        cursor.executemany(sql_insert, data_tuples)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Production Table Created: {PRODUCTION_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load production: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b5400df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 11: Analysis of Cluster Profiles\n",
      "--------------------------------------------------------------------------------\n",
      "ID   Profile Name                        Size       Fatal%   Avg_Exp \n",
      "--------------------------------------------------------------------------------\n",
      "0    High Risk / New Workers             76,700     0.00    % 3.4     y\n",
      "1    Standard Risk / Experienced         64,319     0.00    % 25.6    y\n",
      "2    Intermediate / Severe Injuries      1,124      0.00    % 10.9    y\n",
      "3    High Risk / New Workers             127,584    0.00    % 4.4     y\n",
      "4    High Severity / Fatal Risk          1,194      100.00  % 13.1    y\n",
      "--------------------------------------------------------------------------------\n",
      "Overall Clustering Quality: ACCEPTABLE\n"
     ]
    }
   ],
   "source": [
    "print(\"Cell 11: Analysis of Cluster Profiles\")\n",
    "\n",
    "# 1. Aggregate Statistics per Cluster\n",
    "cluster_stats = df_clean.groupby('CLUSTER').agg({\n",
    "    'IS_FATAL': 'mean',\n",
    "    'SEVERITY_SCORE': 'mean',\n",
    "    'TOT_EXPER': 'mean',\n",
    "    'DOCUMENT_NO': 'count'\n",
    "}).rename(columns={'DOCUMENT_NO': 'Record_Count'})\n",
    "\n",
    "# 2. Automated Labeling Logic (Professional Interpretation)\n",
    "def label_cluster(row):\n",
    "    if row['IS_FATAL'] > 0.02: return \"High Severity / Fatal Risk\"\n",
    "    if row['TOT_EXPER'] < 5:  return \"High Risk / New Workers\"\n",
    "    if row['SEVERITY_SCORE'] > 50: return \"Intermediate / Severe Injuries\"\n",
    "    return \"Standard Risk / Experienced\"\n",
    "\n",
    "cluster_stats['Profile_Name'] = cluster_stats.apply(label_cluster, axis=1)\n",
    "\n",
    "# 3. Presentation Output\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ID':<4} {'Profile Name':<35} {'Size':<10} {'Fatal%':<8} {'Avg_Exp':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in cluster_stats.iterrows():\n",
    "    print(f\"{idx:<4} {row['Profile_Name']:<35} {int(row['Record_Count']):<10,} \"\n",
    "          f\"{row['IS_FATAL']*100:<8.2f}% {row['TOT_EXPER']:<8.1f}y\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Overall Clustering Quality: { 'ACCEPTABLE' if best_k > 2 else 'NEEDS TUNING' }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac95ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12: Final Business Labeling & SQL Production Export\n",
      "Transferring 270,921 records to SQL Server...\n",
      "SUCCESS: Table 'Accidents_Clustered_Final' is ready in OSHE_Analytics.\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "\n",
    "print(\"Step 12: Final Business Labeling & SQL Production Export\")\n",
    "\n",
    "# 1. Interpretasi Bisnis untuk Cluster\n",
    "# Memberikan narasi pada hasil clustering agar informatif di Power BI\n",
    "risk_mapping = {\n",
    "    0: \"Moderate Severity\",\n",
    "    1: \"Low Risk - Operational\", \n",
    "    2: \"CRITICAL - Fatal Risk\",\n",
    "    3: \"Low Risk - Maintenance\"\n",
    "}\n",
    "\n",
    "# Mapping label dan level risiko\n",
    "df_clean['CLUSTER_LABEL'] = df_clean['CLUSTER'].map(risk_mapping)\n",
    "df_clean['RISK_LEVEL'] = df_clean['CLUSTER'].apply(lambda x: 'High' if x == 2 else 'Medium' if x == 0 else 'Low')\n",
    "\n",
    "# 2. Seleksi Kolom Produksi\n",
    "# Hanya membawa kolom yang relevan untuk visualisasi dashboard\n",
    "final_features = [\n",
    "    'MINE_ID', 'DOCUMENT_NO', 'ACCIDENT_DT', 'YEAR', 'MONTH', 'DAY_OF_WEEK', 'SUBUNIT', 'CLASSIFICATION', 'ACCIDENT_TYPE', 'DEGREE_INJURY', 'SEVERITY_SCORE', 'IS_FATAL', 'TOT_EXPER', 'CLUSTER_LABEL', 'RISK_LEVEL', 'FIPS_STATE_CD'\n",
    "]\n",
    "df_final = df_clean[[c for c in final_features if c in df_clean.columns]].copy()\n",
    "\n",
    "# 3. High-Performance SQL Export\n",
    "try:\n",
    "    # PERBAIKAN: Menggunakan get_conn_str(db='db') yang sudah didefinisikan di Cell 1\n",
    "    # fast_executemany=True sangat penting untuk kecepatan insert data besar\n",
    "    conn_params = urllib.parse.quote_plus(get_conn_str(db='db')) \n",
    "    engine = sa.create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_params}\", fast_executemany=True)\n",
    "    \n",
    "    print(f\"Transferring {len(df_final):,} records to SQL Server...\")\n",
    "    \n",
    "    # Export menggunakan SQLAlchemy (Jauh lebih cepat dari pyodbc executemany biasa)\n",
    "    df_final.to_sql(\n",
    "        name='Accidents_Clustered_Final', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        chunksize=25000 \n",
    "    )\n",
    "    \n",
    "    print(f\"SUCCESS: Table 'Accidents_Clustered_Final' is ready in {DB_CONFIG['database']}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SQL Export Failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
