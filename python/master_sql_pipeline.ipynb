{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50447157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded.\n",
      "Project Root: e:\\MyProject-GitHub\\OSHE-Interview-Project\n",
      "Target Table: staging.accidents_raw\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- DATABASE SETTINGS ---\n",
    "DB_CONFIG = {\n",
    "    'server': 'DESKTOP-PG3CATF',\n",
    "    'database': 'OSHE_Analytics',\n",
    "    'driver': 'ODBC Driver 18 for SQL Server'\n",
    "}\n",
    "\n",
    "# Definisikan nama tabel di sini agar terpusat\n",
    "STAGING_TABLE = 'staging.accidents_raw'\n",
    "FINAL_TABLE = 'dbo.Accidents_Clustered_Final'\n",
    "\n",
    "# --- DIRECTORY SETUP ---\n",
    "# Menggunakan Pathlib untuk manajemen folder yang lebih robust\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'python' else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "STAGING_CSV = DATA_DIR / 'staging' / 'accidents_staging.csv'\n",
    "\n",
    "# --- CONNECTION HELPER ---\n",
    "def get_conn(db_name='master'):\n",
    "    \"\"\"Fungsi pembantu untuk membuat koneksi SQL Server secara dinamis\"\"\"\n",
    "    conn_str = (\n",
    "        f\"DRIVER={{{DB_CONFIG['driver']}}};\"\n",
    "        f\"SERVER={DB_CONFIG['server']};\"\n",
    "        f\"DATABASE={db_name};\"\n",
    "        f\"Trusted_Connection=yes;TrustServerCertificate=yes;\"\n",
    "    )\n",
    "    return pyodbc.connect(conn_str)\n",
    "\n",
    "print(f\"Configuration Loaded.\")\n",
    "print(f\"Project Root: {BASE_DIR}\")\n",
    "print(f\"Target Table: {STAGING_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedf0488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking SQL Infrastructure...\n",
      "[Server]: DESKTOP-PG3CATF\n",
      "[Status]: Connected\n",
      "[DB Count]: 5 databases detected\n",
      "Connectivity test passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking SQL Infrastructure...\")\n",
    "\n",
    "try:\n",
    "    with get_conn() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Ambil informasi server dan database yang aktif\n",
    "        cursor.execute(\"SELECT @@SERVERNAME, @@VERSION\")\n",
    "        server, version = cursor.fetchone()\n",
    "        \n",
    "        cursor.execute(\"SELECT name FROM sys.databases\")\n",
    "        db_list = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    print(f\"[Server]: {server}\")\n",
    "    print(f\"[Status]: Connected\")\n",
    "    print(f\"[DB Count]: {len(db_list)} databases detected\")\n",
    "    print(f\"Connectivity test passed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac02f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Database Objects...\n",
      "Database 'OSHE_Analytics' and schema 'staging' are verified/ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Database Objects...\")\n",
    "\n",
    "try:\n",
    "    # 1. Database Creation (Master Context)\n",
    "    conn = get_conn('master')\n",
    "    conn.autocommit = True # Wajib untuk perintah CREATE DATABASE\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Buat database jika belum ada\n",
    "    cursor.execute(f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{DB_CONFIG['database']}') \"\n",
    "                   f\"BEGIN CREATE DATABASE {DB_CONFIG['database']} END\")\n",
    "    conn.close()\n",
    "\n",
    "    # 2. Schema Provisioning (Target DB Context)\n",
    "    with get_conn(DB_CONFIG['database']) as conn:\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Buat schema 'staging' untuk memisahkan data mentah dan data bersih (Best Practice)\n",
    "        cursor.execute(\"IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'staging') \"\n",
    "                       \"EXEC('CREATE SCHEMA staging')\")\n",
    "    \n",
    "    print(f\"Database '{DB_CONFIG['database']}' and schema 'staging' are verified/ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Provisioning failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e00b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Provisioning Staging Infrastructure\n",
      "Staging table 'staging.accidents_raw' created successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Provisioning Staging Infrastructure\")\n",
    "\n",
    "# Pastikan variabel sudah ada di memori\n",
    "if 'STAGING_TABLE' not in globals():\n",
    "    STAGING_TABLE = 'staging.accidents_raw'\n",
    "\n",
    "if not STAGING_CSV.exists():\n",
    "    print(f\"âœ— Error: Data source missing at {STAGING_CSV}\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Analisis Struktur Data (Schema Discovery)\n",
    "        sample_df = pd.read_csv(STAGING_CSV, nrows=1, low_memory=False)\n",
    "        \n",
    "        # 2. Normalisasi Nama Kolom\n",
    "        clean_columns = [c.replace(' ', '_').replace('-', '_').replace('.', '_').replace('(', '').replace(')', '') \n",
    "                         for c in sample_df.columns]\n",
    "\n",
    "        # 3. SQL DDL Generation (Safe Landing Strategy)\n",
    "        col_defs = [f\"[{name}] NVARCHAR(MAX)\" for name in clean_columns]\n",
    "        \n",
    "        create_sql = f\"\"\"\n",
    "        IF OBJECT_ID('{STAGING_TABLE}', 'U') IS NOT NULL DROP TABLE {STAGING_TABLE};\n",
    "        CREATE TABLE {STAGING_TABLE} (\n",
    "            [STG_ID] INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            {', '.join(col_defs)},\n",
    "            [LOAD_TIMESTAMP] DATETIME DEFAULT GETDATE()\n",
    "        )\"\"\"\n",
    "\n",
    "        # Menggunakan koneksi ke OSHE_Analytics\n",
    "        with get_conn(DB_CONFIG['database']) as conn:\n",
    "            conn.autocommit = True\n",
    "            conn.cursor().execute(create_sql)\n",
    "\n",
    "        print(f\"Staging table '{STAGING_TABLE}' created successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create staging infrastructure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5488e674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Executing Data Ingestion Pipeline\n",
      "Ingesting 270,921 records...\n",
      "  Progress: 50,000 / 270,921\n",
      "  Progress: 100,000 / 270,921\n",
      "  Progress: 150,000 / 270,921\n",
      "  Progress: 200,000 / 270,921\n",
      "  Progress: 250,000 / 270,921\n",
      "  Progress: 270,921 / 270,921\n",
      "Ingestion completed. Data is now available in staging.accidents_raw.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Executing Data Ingestion Pipeline\")\n",
    "\n",
    "try:\n",
    "    # 1. Load & Pre-process (Memory Efficient)\n",
    "    df = pd.read_csv(STAGING_CSV, low_memory=False)\n",
    "    \n",
    "    # Sinkronisasi nama kolom dengan tabel SQL (tanpa spasi/karakter spesial)\n",
    "    df.columns = [c.replace(' ', '_').replace('-', '_').replace('.', '_').replace('(', '').replace(')', '') \n",
    "                  for c in df.columns]\n",
    "    \n",
    "    # Konversi seluruh data menjadi string (NVARCHAR compatible) dan handle NULLs\n",
    "    # Mengganti NaN (Not a Number) menjadi None agar masuk sebagai NULL di SQL\n",
    "    df = df.where(pd.notnull(df), None).astype(str).replace('None', None)\n",
    "\n",
    "    # 2. SQL Batch Ingestion\n",
    "    with get_conn(DB_CONFIG['database']) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"TRUNCATE TABLE {STAGING_TABLE}\") # Hapus data lama (Fresh Load)\n",
    "         \n",
    "        # Penyiapan template Query Insert\n",
    "        cols_joined = \", \".join([f\"[{c}]\" for c in df.columns])\n",
    "        placeholders = \", \".join([\"?\"] * len(df.columns))\n",
    "        insert_sql = f\"INSERT INTO {STAGING_TABLE} ({cols_joined}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # 3. Execution in Batches\n",
    "        batch_size = 5000\n",
    "        total_rows = len(df)\n",
    "        print(f\"Ingesting {total_rows:,} records...\")\n",
    "\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch_data = [tuple(x) for x in df.iloc[i : i + batch_size].values]\n",
    "            cursor.executemany(insert_sql, batch_data)\n",
    "            conn.commit()\n",
    "            \n",
    "            if (i + batch_size) % 50000 == 0 or (i + batch_size) >= total_rows:\n",
    "                print(f\"  Progress: {min(i + batch_size, total_rows):,} / {total_rows:,}\")\n",
    "\n",
    "    print(f\"Ingestion completed. Data is now available in {STAGING_TABLE}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ingestion failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
